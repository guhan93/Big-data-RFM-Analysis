{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e67778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark RFM example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7c1306",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = spark.read.format('com.databricks.spark.csv').\\\n",
    "                       options(header='true', \\\n",
    "                       inferschema='true').\\\n",
    "            load(\"Online Retail.csv\",header=True);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bbfae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.show(5)\n",
    "df_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7dbf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "def my_count(df_in):\n",
    "    df_in.agg( *[ count(c).alias(c) for c in df_in.columns ] ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260acaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw.dropna(how='any')\n",
    "my_count(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fcea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_utc_timestamp, unix_timestamp, lit, datediff, col\n",
    "\n",
    "timeFmt = \"MM/dd/yy HH:mm\"\n",
    "\n",
    "df = df.withColumn('NewInvoiceDate'\n",
    "                 , to_utc_timestamp(unix_timestamp(col('InvoiceDate'),timeFmt).cast('timestamp')\n",
    "                 , 'UTC'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7145471",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import round\n",
    "\n",
    "df = df.withColumn('TotalPrice', round( df.Quantity * df.UnitPrice, 2 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeaf0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, min, max, sum, datediff, to_date\n",
    "\n",
    "date_max = df.select(max('NewInvoiceDate')).toPandas()\n",
    "current = to_utc_timestamp( unix_timestamp(lit(str(date_max.iloc[0][0])), \\\n",
    "                              'yy-MM-dd HH:mm').cast('timestamp'), 'UTC' )\n",
    "\n",
    "# Calculatre Duration\n",
    "df = df.withColumn('Duration', datediff(lit(current), 'NewInvoiceDate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e73842d",
   "metadata": {},
   "outputs": [],
   "source": [
    "recency = df.groupBy('CustomerID').agg(min('Duration').alias('Recency'))\n",
    "frequency = df.groupBy('CustomerID', 'InvoiceNo').count()\\\n",
    "                        .groupBy('CustomerID')\\\n",
    "                        .agg(count(\"*\").alias(\"Frequency\"))\n",
    "monetary = df.groupBy('CustomerID').agg(round(sum('TotalPrice'), 2).alias('Monetary'))\n",
    "rfm = recency.join(frequency,'CustomerID', how = 'inner')\\\n",
    "             .join(monetary,'CustomerID', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b1b920",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Recency','Frequency','Monetary']\n",
    "describe_pd(rfm,cols,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a38a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RScore(x):\n",
    "    if  x <= 16:\n",
    "        return 1\n",
    "    elif x<= 50:\n",
    "        return 2\n",
    "    elif x<= 143:\n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "\n",
    "def FScore(x):\n",
    "    if  x <= 1:\n",
    "        return 4\n",
    "    elif x <= 3:\n",
    "        return 3\n",
    "    elif x <= 5:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def MScore(x):\n",
    "    if  x <= 293:\n",
    "        return 4\n",
    "    elif x <= 648:\n",
    "        return 3\n",
    "    elif x <= 1611:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, DoubleType\n",
    "\n",
    "R_udf = udf(lambda x: RScore(x), StringType())\n",
    "F_udf = udf(lambda x: FScore(x), StringType())\n",
    "M_udf = udf(lambda x: MScore(x), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c42502",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_seg = rfm.withColumn(\"r_seg\", R_udf(\"Recency\"))\n",
    "rfm_seg = rfm_seg.withColumn(\"f_seg\", F_udf(\"Frequency\"))\n",
    "rfm_seg = rfm_seg.withColumn(\"m_seg\", M_udf(\"Monetary\"))\n",
    "rfm_seg.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4b15d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_seg = rfm_seg.withColumn('RFMScore',\n",
    "                             F.concat(F.col('r_seg'),F.col('f_seg'), F.col('m_seg')))\n",
    "rfm_seg.sort(F.col('RFMScore')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a4f9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_seg.groupBy('RFMScore')\\\n",
    "       .agg({'Recency':'mean',\n",
    "             'Frequency': 'mean',\n",
    "             'Monetary': 'mean'} )\\\n",
    "        .sort(F.col('RFMScore')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e38575b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grp = 'RFMScore'\n",
    "num_cols = ['Recency','Frequency','Monetary']\n",
    "df_input = rfm_seg\n",
    "\n",
    "quantile_grouped = quantile_agg(df_input,grp,num_cols)\n",
    "quantile_grouped.toPandas().to_csv(output_dir+'quantile_grouped.csv')\n",
    "\n",
    "deciles_grouped = deciles_agg(df_input,grp,num_cols)\n",
    "deciles_grouped.toPandas().to_csv(output_dir+'deciles_grouped.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0bc2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "\n",
    "def transData(data):\n",
    "    return data.rdd.map(lambda r: [r[0],Vectors.dense(r[1:])]).toDF(['CustomerID','rfm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed1a2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed= transData(rfm)\n",
    "transformed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ad7f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(inputCol=\"rfm\",\\\n",
    "         outputCol=\"features\")\n",
    "scalerModel =  scaler.fit(transformed)\n",
    "scaledData = scalerModel.transform(transformed)\n",
    "scaledData.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97b04ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.sql.functions import col, percent_rank, lit\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import DataFrame, Row\n",
    "from pyspark.sql.types import StructType\n",
    "from functools import reduce  \n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "import numpy as np\n",
    "cost = np.zeros(20)\n",
    "for k in range(2,20):\n",
    "    kmeans = KMeans()\\\n",
    "            .setK(k)\\\n",
    "            .setSeed(1) \\\n",
    "            .setFeaturesCol(\"features\")\\\n",
    "            .setPredictionCol(\"cluster\")\n",
    "\n",
    "    model = kmeans.fit(scaledData)\n",
    "    cost[k] = model.computeCost(scaledData) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea82244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sbs\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize =(8,6))\n",
    "ax.plot(range(2,20),cost[2:20], marker = \"o\")\n",
    "ax.set_xlabel('k')\n",
    "ax.set_ylabel('cost')\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86820d32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
